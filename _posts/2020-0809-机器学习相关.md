---
layout:     post			    # 使用的布局（不需要改）
title:      机器学习相关 				# 标题 
subtitle:   面试
date:       2020-08-09			# 时间
author:     Tao				# 作者
header-img: img/post-bg-universe.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 算法
---

# 分类模型

## 逻辑回归LR
LR: Logistic Regression

逻辑回归常被用于估计一个事物属于某个类别的概率。实际中最常用的就是二分类的Logistic回归。

$$ h_\theta(x) = g(\theta^T x) = \frac{1}{1+e^{-\theta^Tx}}$$
这里
$h_\theta(x)$
表示结果取1的概率，所以：
$$ P(y=1|x,\theta)=h_\theta(x) $$
$$ P(y=0|x,\theta)=1-h_\theta(x) $$

LR最基本的学习算法是最大似然，似然函数就是可能，概率函数

假设有n个样本，因为各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积。得到似然函数为
$$ L(\theta)=\prod_{i=1}^{m} h_\theta(x_i)^{y_i}\left[ 1-h_\theta(x_i)\right]^{1-y_i}$$

然后目的是为了求解似然函数最大值，对上式两边取对数
$$ \log[L(\theta)]=\sum_{i=1}^{m} y_ilog[h_\theta(x_i)]+(1-y_i)log[1-h_\theta(x_i)]$$

所以，我们就可以构造Loss Functionl如下式，前面加上一个负数，这样求loss最小等于求似然函数最大，这样做是为了方便使用梯度下降算法。

$$ J(\theta)=-\frac{1}{m}\log[L(\theta)]=-\frac{1}{m}\sum_{i=1}^{m} y_ilog[h_\theta(x_i)]+(1-y_i)log[1-h_\theta(x_i)]$$

梯度下降算法更新函数为：
$$ \theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta}J(\theta) $$
$$ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^m[h_\theta(x_i)-y_i]x_i $$

化简可以见[这里](https://blog.csdn.net/iteye_12028/article/details/82570835?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param)


## 决策树

## 感知机
感知机是二类分类的线性分类模型，对应于输入空间中将实例分类正负两类的超平面。
$$ f(x) = sign(w^Tx) $$ 
$$ sign(a) = \begin{cases} +1, & a \ge 0 \ -1, & a < 0 \end{cases} $$

感知机中采用的损失函数是误分类点到超平面S的总距离。
输入样本中的任意一点到超平面S的距离： 
$$ L = \frac{1}{||w||}|w^T x_i|$$

$\qquad ||w||$是w的L_2范数

那么误分类点到超平面 S 的距离为：
$$\frac{1}{||w||}y_i(w^T x_i) $$
$\frac{1}{||w||}$ 
是一个固定值，可以不用考虑，这样我们也可以得出我们的损失函数如下： 
$$ L(w,b) = - \sum_{x_i \in M }{y_i w^Tx_i } $$

损失函数对于
$w$
方向上的梯度为： 
$$ \frac{\partial {L}}{\partial{w}} = - \sum_{x_i \in M} y_i x_i $$
那么随便给定一个误分类点
$(x_i, y_i)$
， 对w，b的更新如下： 
$$ w \leftarrow w + \eta y_i x_i $$

## 朴素贝叶斯
### 基本概念

1. 条件概率
$$ P(X|Y) = \frac{P(X,Y)}{P(Y)} $$
$P(X|Y)$含义： 表示 y 发生的条件下 x 发生的概率。

2. 先验概率

**表示事件发生前的预判概率。** 这个可以是基于历史数据统计，也可以由背景常识得出，也可以是主观观点得出。一般都是单独事件发生的概率，如 P(A)

3. 后验概率

基于先验概率求得的反向条件概率，形式上与条件概率相同（若 P(X|Y) 为正向，则 P(Y|X) 为反向）

### 贝叶斯公式
$$ P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)} \ $$

- P(Y) 叫做先验概率，意思是事件X发生之前，我们对事件Y发生的一个概率的判断
- P(Y|X) 叫做后验概率，意思是时间X发生之后，我们对事件Y发生的一个概率的重新评估
- P(Y,X) 叫做联合概率， 意思是事件X与事件Y同时发生的概率。

### 条件独立假设
$$ P(x|c) = p(x_1, x_2, \cdots x_n | c) = \prod_{i=1}^Np(x_i | c) $$
朴素贝叶斯采用条件独立假设的动机在于： 简化运算。

### 从机器学习视角理解朴素贝叶斯
朴素贝叶斯 = 贝叶斯方法 + 条件独立假设。

在机器学习中，我们可以将 X 理解为“具有某特征”， 而 Y 理解为“类别标签”，于是有： 
$$ P("属于某类"|"具有某特征")=\frac{P("具有某特征"|"属于某类")P("属于某类")}{P("具有某特征")$$

而贝叶斯派目的是要对 Y 进行优化：
$$ Y_{MAP}=argmax_y P(Y|X) = argmax_y \frac{P(X,Y)}{P(X)} = argmax_y P(Y) P(X|Y) $$

### 朴素贝叶斯中的三种模型
1. 多项式模型适用于离散特征情况，在文本领域应用广泛， 会做一些平滑处理,其基本思想是：我们将重复的词语视为其出现多次。
2. 当特征是连续变量的时候(比如身高体重)，运用多项式模型就会导致很多0,高斯模型假设每一维特征都服从高斯分布（正态分布）
3. 伯努利模型，与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).

## 随机森林
### bagging
- 思想：总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。

### 随机森林
随机森立是 Bagging 的优化版本。其包含的思想在于： 随机选择样本数建立多个训练集并随机选取特征集合，根据多个训练集与特征集合来建立多颗决策树，然后进行投票决策。

随机森林的最终目的是建立 m 颗决策树，而每颗决策树的建立过程如下：
- 如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本，作为该树的训练集。
- 如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的
- 每棵树都尽最大程度的生长，并且没有剪枝过程。

m 是随机森林中唯一的一个参数。
- 减小特征选择个数m，树的相关性和分类能力也会相应的降低
- 增大m，两者也会随之增大。

## SVM
### 简介 [link](https://zhuanlan.zhihu.com/p/77750026)  [link2](https://github.com/songyingxin/NLPer-Interview/blob/master/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B6%20-%20SVM.md)
- SVM 三宝： 间隔，对偶，核技巧。它属于判别模型。Support Vector Machine
- **支持向量：** 在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。
- 支持向量机（SVM）：其含义是通过支持向量运算的分类器。
- SVM 是一种二分类模型， 它的目的是寻找一个超平面来对样本进行分割，分割的依据是间隔最大化，最终转化为一个凸二次规划问题来求解。

# 回归模型
## 线性回归

## 岭回归

# 基础知识
## SVM核方法

## 判别模型/生成模型

## 集成学习

## 频率派/贝叶斯流派

# 模型
## 概率图HMM

## KNN

## LDA

## 树回归

## 聚类算法

# 降维
## PCA降维

# 集成学习
## GBDT

## LightGBM

## XGBoost

