---
layout:     post			    # 使用的布局（不需要改）
title:      面试题基础 				# 标题 
subtitle:   面试
date:       2020-07-20			# 时间
author:     Tao				# 作者
header-img: img/post-bg-universe.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 算法
---

### 编程

```python
（1）Leetcode 3、5
（2）leetcode 139
（3）leetcode 140
（4）leetcode 32
（5）leetcode 20
（6）leetcode 324 Wiggle Sort II
（7）leetcode 448、121、123
（8）leetcode 125、680
（9）leetcode 153、154
（10）最小编辑距离（动规、加权）
（11）两个有序数组求交集
（12）leetcode 1143 Longest Common Subsequence 最大公共子序列和最大公共子串（子序列可不连续，子串必须连续）
（13）topK小，第K大。怎么最快的找到一堆大数组里面第10大的数。n个数字的流（只能看一次），求前k大（O（N））。
（14）如何判断链表是否有环
（15）二叉树最大深度
（16）地图中找出大陆的个数（一道BFS题）

leetcode 48 旋转图像 
470 用rand7实现rand10
剑指offer 顺时针打印矩阵
200 463 695 岛屿数量/最大面积等岛屿问题
```
- 实现kmeans、KNN
```python
"""
其伪代码如下：

创建k个点作为初始的质心点（随机选择）
当任意一个点的簇分配结果发生改变时
       对数据集中的每一个数据点
              对每一个质心
                     计算质心与数据点的距离
              将数据点分配到距离最近的簇
       对每一个簇，计算簇中所有点的均值，并将均值作为质心
"""
from numpy import *
import time
import matplotlib.pyplot as plt

# calculate Euclidean distance
def euclDistance(vector1, vector2):
	return sqrt(sum(power(vector2 - vector1, 2)))
 
# init centroids with random samples
def initCentroids(dataSet, k):
	numSamples, dim = dataSet.shape
	centroids = zeros((k, dim))
	for i in range(k):
		index = int(random.uniform(0, numSamples))
		centroids[i, :] = dataSet[index, :]
	return centroids
 
# k-means cluster
def kmeans(dataSet, k):
	numSamples = dataSet.shape[0]
	# first column stores which cluster this sample belongs to,
	# second column stores the error between this sample and its centroid
    clusterAssment = mat(zeros((numSamples, 2)))
	clusterChanged = True
 
	## step 1: init centroids
	centroids = initCentroids(dataSet, k)
 
	while clusterChanged:
		clusterChanged = False
		## for each sample
		for i in xrange(numSamples):
			minDist  = 100000.0
			minIndex = 0
			## for each centroid
			## step 2: find the centroid who is closest
			for j in range(k):
				distance = euclDistance(centroids[j, :], dataSet[i, :])
				if distance < minDist:
					minDist  = distance
					minIndex = j
			
			## step 3: update its cluster
			if clusterAssment[i, 0] != minIndex:
				clusterChanged = True
				clusterAssment[i, :] = minIndex, minDist**2
 
		## step 4: update centroids
		for j in range(k):
			pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]]
			centroids[j, :] = mean(pointsInCluster, axis = 0)
 
	print 'Congratulations, cluster complete!'
```

```python
"""
一般情况下，kNN有如下流程：
（1）收集数据：确定训练样本集合测试数据；
（2）计算测试数据和训练样本集中每个样本数据的距离；(常用的距离计算公式：欧式距离公式 曼哈顿距离公式)
（3）按照距离递增的顺序排序；
（4）选取距离最近的k个点；
（5）确定这k个点中分类信息的频率；
（6）返回前k个点中出现频率最高的分类，作为当前测试数据的分类。
"""

from numpy import *
import operator

#定义KNN算法分类器函数
#函数参数包括：(测试数据，训练数据，分类,k值)
def classify(inX,dataSet, labels, k):
    dataSetSize = dataSet.shape[0]
    diffMat = tile(inX,(dataSetSize,1))-dataSet
    sqDiffMat=diffMat**2
    sqDistances=sqDiffMat.sum(axis=1)
    distances=sqDistances**0.5 #计算欧式距离
    sortedDistIndicies=distances.argsort() #排序并返回index
    #选择距离最近的k个值
    classCount={}
    for i in range(k):
        voteIlabel=labels[sortedDistIndicies[i]]
        #D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.
        classCount[voteIlabel]=classCount.get(voteIlabel,0)+1
    #排序
    sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)
    return sortedClassCount[0][0]

#定义一个生成“训练样本集”的函数，包含特征和分类信息
def createDataSet():
    group=array([[1,1.1],[1,1],[0,0],[0,0.1]])
    labels=['A','A','B','B']
    return group,labels

import KNN
#生成训练样本
group,labels=KNN.createDataSet()
#对测试数据[0,0]进行KNN算法分类测试
KNN.classify([0,0],group,labels,3)
Out[3]: 'B'
```

- 从一个素数到另一个素数有几种方式？

- 前缀树实现（精确、模糊匹配）
```python
trie树，又叫字典树，前缀树，单词查找树，键树，是一种多叉结构。

性质：
1. 根结点不包含字符，其他节点都包含字符
2. 从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串
3. 每个节点的所有子节点包含的字符互不相同

通常在实现的时候，会在节点结构中设置一个标志，用来标记该结点处是否构成一个单词（关键字）。

两个有公共前缀的关键字，在Trie树中前缀部分的路径相同，所以Trie树又叫做前缀树（Prefix Tree）

Trie树的核心思想是空间换时间，利用字符串的公共前缀来减少无谓的字符串比较以达到提高查询效率的目的。

应用：字符串检索，词频统计，字符串排序，前缀匹配，辅助结构
```
```cpp
#define ALPHABET_SIZE 26

typedef struct trie_node
{
	int count;   // 记录该节点代表的单词的个数
	trie_node *children[ALPHABET_SIZE]; // 各个子节点 
}*trie;

trie_node* create_trie_node()
{
	trie_node* pNode = new trie_node();
	pNode->count = 0;
	for(int i=0; i<ALPHABET_SIZE; ++i)
		pNode->children[i] = NULL;
	return pNode;
}

void trie_insert(trie root, char* key)
{
	trie_node* node = root;
	char* p = key;
	while(*p)
	{
		if(node->children[*p-'a'] == NULL)
		{
			node->children[*p-'a'] = create_trie_node();
		}
		node = node->children[*p-'a'];
		++p;
	}
	node->count += 1;
}
/**
 * 查询：不存在返回0，存在返回出现的次数
 */ 
int trie_search(trie root, char* key)
{
	trie_node* node = root;
	char* p = key;
	while(*p && node!=NULL)
	{
		node = node->children[*p-'a'];
		++p;
	}
	
	if(node == NULL)
		return 0;
	else
		return node->count;
}

int main()
{
	// 关键字集合
	char keys[][8] = {"the", "a", "there", "answer", "any", "by", "bye", "their"};
	trie root = create_trie_node();

	// 创建trie树
	for(int i = 0; i < 8; i++)
		trie_insert(root, keys[i]);

	// 检索字符串
	char s[][32] = {"Present in trie", "Not present in trie"};
	printf("%s --- %s\n", "the", trie_search(root, "the")>0?s[0]:s[1]);

	return 0;
}
```


- 地图中找出大陆的个数（一道BFS题）
- 二叉树最大深度
```python
# DFS
class Solution:
    def maxDepth(self, root: TreeNode) -> int:
        if root is None:
            return 0
        return 1 + max(self.maxDepth(root.left), self.maxDepth(root.right))
```
```python
# BFS
class Solution:
    def maxDepth(self, root: TreeNode) -> int:
        if root is None:
            return 0
        from collections import deque

        queue = deque()
        queue.append(root)
        depth = 0
        while len(queue) > 0:
            depth += 1
            for _ in range(len(queue)):
                node = queue.popleft()
                if node.left is not None:
                    queue.append(node.left)
                if node.right is not None:
                    queue.append(node.right)          
        return depth
```
二叉树最小深度 注意细节
```python
class Solution:
    def minDepth(self, root: TreeNode) -> int:
        def dp(node):
            if node is None:
                return 0

            left = dp(node.left)
            right = dp(node.right)
            if left == 0 and right == 0:
                return 1
            elif left == 0 or right == 0:
                return left + right + 1
            else:
                return 1 + min(left, right)

        return dp(root)
```
- 如何判断链表是否有环
- topK小，第K大。怎么最快的找到一堆大数组里面第10大的数。n个数字的流（只能看一次），求前k大（O（N））

### 数学相关
- （1）贝叶斯计算概率？
- （2）25只兔子赛跑问题，共5个赛道，最少几次比赛可以选出前5名？
- （3）100盏灯问题？
- x, y服从0-1均匀分布，求x+y<1的概率？x, y, z服从0-1均匀分布，求x+y+z<1的概率？
- （5）x, y是独立的随机变量，方差期望已知，那么如何求 xy 的方差
- （6）矩阵乘法怎么优化？
- （7）两个上三角矩阵相乘如何优化？
- 什么是半正定矩阵？机器学习中有什么应用？


### 机器学习
#### 基础
- LR、SVM、最大熵、决策树
- SVM原理，与感知机的区别
- HMM 的假设？解决了什么问题？HMM中的矩阵意义？
- CRF的假设是什么？解决了什么问题？CRF做特征工程？维特比算法
- CRF和HMM对比，概率表达不同，递推的公式

#### 聚类
- k-means、KNN，其中KNN算法是否可微
- 层次聚类
- LDA、pLSA、LSA

#### 模型集成
- 为什么要做模型集成？
- boosting两种方法，bagging，stacking
- Boosting、Bagging、随机森林
- 4.Xgboost、Adaboost

#### 其它
- PageRank、TextRank
- 主流推荐算法

### 深度学习
#### 基础
- 常用loss函数、公式、适用场景，尤其交叉熵，高频考点
- 梯度消失和梯度爆炸 产生原因？如何解决？
- 过拟合 产生原因？解决方法
- 正向传播和反向传播
- dropout的作用
- L1、L2范数数学公式，及它们的作用
- 常见激活函数及优缺点 sigmoid, tanh, ReLU
- SoftMax + CrossEntropy的反向梯度求导
- 怎么解决beam-search局部最优问题？global embedding 怎么做？
- sgd与adam的区别

#### 神经网络
- 卷积的物理意义是什么？傅里叶变换是什么？
CNN在文本中的用法，pooling的作用，有哪些pooling
- RNN/LSTM
（1）基本框架结构、三个门是什么？计算公式，实际使用时LSTM维度如何选？
（2）rnn梯度弥散和爆炸的原因，lstm为什么不会这样
（3）RNN和LSTM对比
（4）LSTM和GRU的区别
- Transformer
Transformer在实际中怎么用，各部分怎么用？
self-attention原理公式和应用、什么情况下要用
K、Q、V分别是啥，如何计算，
- Attention
Attention怎么用？
- 说一下transforemr、LSTM、CNN几个之间的区别？从多个角度进行讲解？
- Bert
结构、网络层数（12或24层）
预训练：训练任务、过程
如何应用（feature-based、fine-tunning）
- ELMo
- fastText
（1）FastText结构，训练预测为什么快？（因为：层次softmax和负采样，具体原理如哈夫曼树如何构建）
（2）如何获得可靠的top k的输出（第一个0.99，第二个0.001这样的概率很不合理）
- ELMo、GPT、Bert对比
- 神经网络优化的难点是什么？
- pytorch的代码流程

### NLP相关
#### 向量
- 有哪些？如何训练？各有什么优势劣势？对比？
- Word2vec、Glove、fastText
- word2vec如何训练，CBOW和skip-gram结构理解， hierarhical softmax和negative sampling原理、作用。
- word2vec和fastText对比？
注意fastText训练词向量用的是skip-gram模型，做文本分类用的是CBOW模型，分开做对比。
- glove和word2vec、 LSA对比

#### 语言模型
- 传统n-gram原理、和Word2vec对比
- 神经网络语言模型原理？有哪些神经网络语言模型？
- 传统的统计语言模型和神经网络语言模型有什么不同？

#### 多轮对话
- 如何实现，内部状态机如何设计、如何做到可配置、状态学习（learning）

#### 分类任务
- 层级分类、多标签分类

#### 序列标注任务
- 在做NER任务时，lstm后面可以不用加CRF吗？

#### 相似度
- word2vec和tf-idf 相似度计算时的区别？

#### 具体任务场景
- 自动文章摘要抽取时，怎么对一篇文章进行分割？（从序列标注、无监督等角度思考）